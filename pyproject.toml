[project]
name = "kiro-insbridge"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12, <3.13"
dependencies = [
    
]

[dependency-groups]
engine = [
    "pydantic-settings>=2.8.1",
    "tabulate>=0.9.0",
    "cloudpickle>=3.1.1",
    "pylint>=3.3.6",
    "xmltodict",
    "pathlib",
    "xml_utils",
    "absl-py",
    "pyyaml",
    "loguru==0.7.3",
    "jsonschema",
    "jinja2",
]

# airflow = [
    # airflow must be <3 in order for datahub's integration to work as of 2025-07-01
    # "apache-airflow>=2.7.0,<3",
    # "apache-airflow-providers-amazon>=9.9.0",
# ]
datahub = [
    "acryl-datahub>=1.1.0.4",
]
prefect = [
    "prefect>=3.0.0",
    "boto3>=1.28.0",
    "pyzipper>=0.3.6",
    "ipykernel>=6.29.5",
    "rich>=13.9.4",
    "pytest>=8.3.5",
    "pytest-mock>=3.14.0",
    "scikit-learn>=1.6.1",
    "pytest-cov>=6.0.0",
    "pytest-asyncio>=0.25.3",
    "flake8-pyproject>=1.2.3",
    "pylint>=3.3.6",
    "pyink>=24.10.1",
    "google-cloud-aiplatform[evaluation]>=1.88.0",
    "pre-commit",
    "types-xmltodict",
    "types-PyYAML",
    "types-jsonschema",
]
cdk = [
    "aws-cdk-lib>=2.204.0",
    "constructs>=10.4.2",
]

dev = [
    # Engine dependencies
    "pydantic-settings>=2.8.1",
    "tabulate>=0.9.0",
    "cloudpickle>=3.1.1",
    "pylint>=3.3.6",
    "xmltodict",
    "pathlib",
    "xml_utils",
    "absl-py",
    "pyyaml",
    "loguru==0.7.3",
    "jsonschema",
    "jinja2",
    # Prefect dependencies
    "prefect>=3.0.0",
    "boto3>=1.28.0",
    "pyzipper>=0.3.6",
    "ipykernel>=6.29.5",
    "rich>=13.9.4",
    "pytest>=8.3.5",
    "pytest-mock>=3.14.0",
    "scikit-learn>=1.6.1",
    "pytest-cov>=6.0.0",
    "pytest-asyncio>=0.25.3",
    "flake8-pyproject>=1.2.3",
    "pyink>=24.10.1",
    "google-cloud-aiplatform[evaluation]>=1.88.0",
    "pre-commit",
    "types-xmltodict",
    "types-PyYAML",
    "types-jsonschema",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# https://github.com/astral-sh/ruff?tab=readme-ov-file#configuration
[tool.ruff]
line-length = 119

# https://docs.astral.sh/ruff/rules/
[tool.ruff.lint]
extend-select = [
    # pycodestyle errors
    "E",
    # flake8-bugbear
    "B",
    # pylint equivalent rules
    "PL",
    # isort
    "I",
    # pydocstyle for docstrings
    "D",
    # pep8-naming
    "N",
]
ignore = [
    "E501", # line-too-long
    "F401", # unused-import
    "W605", # invalid-escape-sequence
    "W291", # trailing-whitespace"
    "UP032", # Use f-string instead of format call
    # pydocstyle - https://docs.astral.sh/ruff/rules/#pydocstyle-d
    "D103", # undocumented-public-function
    "D104", # Missing docstring in public package
    "D107", # Missing docstring in `__init__`
    "D203", # incorrect-blank-line-before-class
    "D100", # undocumented-public-module
    "D101", # Missing docstring in public class
    "D102", # Missing docstring in public method
    "D202", # No blank lines allowed after function docstring
    "D205", # 1 blank line required between summary line and description
    "D212", # multi-line-summary-first-line
    "D400", # first line should end with a period
    "D401", # first line should be in imperative mood
    "D415", # First line should end with a period, question mark, or exclamation point
    # https://docs.astral.sh/ruff/rules/#pylint-pl
    "PLR2004", # Magic value used in comparison, consider replacing `2` with a constant variable
    # Magic value used in comparison, consider replacing `4` with a constant variable -- Is problem for tests
    "PLR2004",
    # `with` statement variable `tmp_dir` overwritten by assignment target, eg: tests/unit_tests/components/metaflow_project/test__metaflow_project_.py:17:9
    "PLW2901",
    "PLC0415", # import should be at the top level of a file
    "PLR0913", # Too many arguments in function definition
]
[tool.ruff.lint.pydocstyle]
convention = "google"


[tool.poe]
executor.type = "uv"

# Airflow Docker tasks
[tool.poe.tasks.airflow-docker]
help = "Start Airflow in Docker"
cmd = "./run airflow-docker"

[tool.poe.tasks.airflow-docker-datazone]
help = "Start Airflow in Docker with Amazon DataZone backend"
cmd = "./run airflow-docker-datazone"

[tool.poe.tasks.clean]
help = "Remove all Artifacts"
shell = """
rm -rf dist build coverage.xml test-reports
rm -rf "./airflow/logs/" "./airflow/airflow.db" "./airflow/airflow.cfg"

find . \\
    -type d \\
    \\( \\
    -name "*cache*" \\
    -o -name "*.dist-info" \\
    -o -name "*.egg-info" \\
    -o -name "*htmlcov" \\
    -o -name "*.metaflow" \\
    -o -name "*.metaflow.s3" \\
    -o -name "*.mypy_cache" \\
    -o -name "*.pytest_cache" \\
    -o -name "*.ruff_cache" \\
    -o -name "*__pycache__" \\
    \\) \\
    -not -path "*env/*" \\
    -not -path "*venv/*" \\
    -not -path "*/.venv/*" \\
    -not -path "*/node_modules/*" \\
    -exec rm -rf {} + 2>/dev/null || true

find . \\
    -type f \\
    \\( \\
    -name "*.pyc" \\
    -o -name "*.DS_Store" \\
    -o -name "*.coverage*" \\
    \\) \\
    -not -path "*env/*" \\
    -not -path "*venv/*" \\
    -not -path "*/.venv/*" \\
    -not -path "*/node_modules/*" \\
    -exec rm -f {} + 2>/dev/null || true
"""

[tool.hatch.build.targets.wheel]
packages = ["src/kiro_insbridge"]

[tool.uv.sources]
ds-dqv-tool = { git = "https://github.com/patterninc/ds-dqv-tool.git" }
THIS_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"

export AWS_PROFILE=sandbox
export AWS_REGION=us-east-1
export AWS_DEFAULT_REGION=${AWS_REGION}

# aws account id/hash used to make a unique, but consistent bucket name
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --profile $AWS_PROFILE --query "Account" --output text)
export AWS_ACCOUNT_ID_HASH=$(echo -n "${AWS_ACCOUNT_ID}" | sha256sum | cut -c5-8)


# remove all files generated by tests, builds, or operating this codebase
function clean {
    rm -rf dist build coverage.xml test-reports sample/ tests/cookiecutter*json
    rm -rf "${THIS_DIR}/airflow/logs/"
    rm -rf "${THIS_DIR}/airflow/airflow.db"
    rm -rf "${THIS_DIR}/airflow/airflow.cfg"
    find . \
      -type d \
      \( \
        -name "*cache*" \
        -o -name "*.dist-info" \
        -o -name "*.egg-info" \
        -o -name "*htmlcov" \
        -o -name "*.metaflow" \
        -o -name "*.metaflow.s3" \
        -o -name "*.mypy_cache" \
        -o -name "*.pytest_cache" \
        -o -name "*.ruff_cache" \
        -o -name "*__pycache__" \
      \) \
      -not -path "*env/*" \
      -exec rm -r {} + || true

    find . \
      -type f \
      -name "*.pyc" \
      -o -name "*.DS_Store" \
      -o -name "*.coverage*" \
      -not -path "*env/*" \
      -exec rm {} +

    airflow db reset -y
}

######### Airflow #########

function airflow-docker() {
    pushd "${THIS_DIR}/airflow"
    docker compose up --build
    popd
}

function airflow() {
    # Managing Airflow Variables using env vars: https://airflow.apache.org/docs/apache-airflow/stable/howto/variable.html
    export AIRFLOW_HOME="${THIS_DIR}/airflow"
    export AIRFLOW__CORE__LOAD_EXAMPLES=False
    export AWS_PROFILE=sandbox
    export AWS_REGION=us-east-1
    export AWS_DEFAULT_REGION=us-east-1
    uv run airflow ${@}
}

function init-airflow() {
    # init airflow sqlite db if not present
    [ -f "${THIS_DIR}/airflow/airflow.db" ] || airflow db reset -y
    # Set Airflow variables
    airflow variables set datalake-aws-region "${AWS_REGION}"
    airflow variables set datalake-s3-bucket "${S3_DATA_LAKE_BUCKET_NAME}"
    airflow variables set datalake-glue-database "${GLUE_DATABASE}"

    airflow connections add  --conn-type 'datahub-rest' 'datahub_rest_default' --conn-host 'http://localhost:8080'

    airflow users create \
        --username test \
        --password test \
        --firstname test \
        --lastname test \
        --role Admin \
        --email test@test.com
}

function start-airflow() {
    airflow standalone
}
